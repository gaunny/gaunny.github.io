[{"title":"Bert模型与论文理解","url":"/2020/01/31/Bert模型与论文理解/","content":"论文解读与Bert模型学习\n\n\n\n#### Bert太火了，大创用到的模型就是bert，但是之前自己作为负责人一直在忙其他工作，没有将重心放在模型学习与理解上，恰好寒假，我有时间好好读一下论文，先将整理到的相关资源放在下面：\n\n### 1.Google官方：\n\n（1）[BERT：Pre-training of Deep Bidirectional Transformers for Language Understanding (BERT:语言理解的深度双向变换器预训练)](https://arxiv.org/abs/1810.04805)\n\n（2）GitHub:[源代码](https://github.com/google-research/bert)\n\n（3）\n\n### 2.大佬笔记\n\n（1）[张俊林](https://zhuanlan.zhihu.com/p/49271699)\n\n（2）[李如](https://zhuanlan.zhihu.com/p/46652512)\n\n## BERT模型：\n\n​\tBERT的全称是Bidirectional Encoder Representation from Tansformers，即双向Transformer的Encoder，因为decoder是不能获取要预测的信息的，模型的主要创新点都在pre-train方法上，即用了Masked LM和Next Sentence Prediction两种方法分别捕捉词语与句子级别的representation。\n\n### 理解模型之前，要知道：\n\n### 1、[Attention](https://zhuanlan.zhihu.com/p/43493999)原理\n\n#### （1）核心思想\n\n​\t在decoding阶段对input中的信息赋予不同权重。在NLP中就是针对sequence的每个time step input，在cv中就是针对每个pixel。\n\n#### （2）原理解析\n\n​\t比较基础的加入attention与rnn结合的model是：（也叫soft attention)\n\n\n\n![preview](https://pic4.zhimg.com/v2-5a509cc5d422b5d83006f41738dd7b43_r.jpg)\n\n![img](https://pic4.zhimg.com/80/v2-f0a7c907fca9301a628ac3a5bfe04ac7_hd.jpg)\n\n\n\n​\t其中 ![[公式]](https://www.zhihu.com/equation?tex=%5Calpha_%7B0%7D%5E1) 是 ![[公式]](https://www.zhihu.com/equation?tex=h_%7B0%7D%5E1)\n\n对应的权重，算出所有权重后会进行softmax和加权，得到![[公式]](https://www.zhihu.com/equation?tex=c%5E0)\n\n可以看到Encoding和decoding阶段仍然是rnn，但是decoding阶使用attention的输出结果 ![[公式]](https://www.zhihu.com/equation?tex=c%5E0%2C+c%5E1) 作为rnn的输入。\n\n**那么重点来了， 权重 ![[公式]](https://www.zhihu.com/equation?tex=%5Calpha) 是怎么来的呢？常见有三种方法：**\n\n- ![[公式]](https://www.zhihu.com/equation?tex=%5Calpha_%7B0%7D%5E1%3Dcos%5C_sim%28z_0%2C+h_1%29)\n- ![[公式]](https://www.zhihu.com/equation?tex=%5Calpha_0+%3Dneural%5C_network%28z_0%2C+h%29)\n- ![[公式]](https://www.zhihu.com/equation?tex=%5Calpha_0+%3D+h%5ETWz_0)\n\n思想就是根据当前解码“状态”判断输入序列的权重分布。\n\n如果把attention剥离出来去看的话，其实是以下机制：\n\n![preview](https://pic3.zhimg.com/v2-99839119705a82a807409d104f63e88a_r.jpg)\n\n输入是query(Q), key(K), value(V)，输出是attention value。如果与之前的模型对应起来的话，query就是 ![[公式]](https://www.zhihu.com/equation?tex=z_0%2C+z_1) ，key就是 ![[公式]](https://www.zhihu.com/equation?tex=h_1%2C+h_2%2C+h_3%2C+h_4) ，value也是![[公式]](https://www.zhihu.com/equation?tex=h_1%2C+h_2%2C+h_3%2C+h_4)。模型通过Q和K的匹配计算出权重，再结合V得到输出：\n\n![[公式]](https://www.zhihu.com/equation?tex=Attention%28Q%2C+K%2C+V%29+%3D+softmax%28sim%28Q%2C+K%29%29V+%5C%5C)\n\n再深入理解下去，这种机制其实做的是**寻址（addressing）**，也就是模仿中央处理器与存储交互的方式将存储的内容读出来，可以看一下[李宏毅老师的课程](https://link.zhihu.com/?target=http%3A//speech.ee.ntu.edu.tw/~tlkagk/courses_MLSD15_2.html)。\n\n#### （3）模型分类\n\n##### 3.1 Soft/Hard Attention\n\nSoft Attention：传统attention。可被嵌入到模型中去进行训练并传播梯度\n\nHard Attention：不计算所有输出，依据概率对encoder的输出采样，在反向传播时需采用[蒙特卡洛进行梯度估计(MCGE)](https://zhuanlan.zhihu.com/p/81806149)\n\n##### 3.2 Global/Local Attention\n\nGlobal Attention：传统attention，对所有encoder输出进行计算\n\nLocal Attention：介于soft和hard之间，会预测一个位置并选取一个窗口进行计算\n\n##### 3.3 Self Attention\n\n传统attention是计算Q和K之间的依赖关系，而self attention则分别计算Q和K自身的依赖关系。\n\n#### （4）优缺点\n\n##### 优点：\n\n·在输出序列与输入序列“顺序”不同的情况下表现较好，如翻译、阅读理解\n\n·相比RNN可以编码更长的序列信息\n\n##### 缺点：\n\n·对序列顺序不敏感\n\n·通常和RNN结合使用，不能并行化\n\n#### （5）TF源码解析\n\n[tensorflow源码解读：Attention Seq2Seq模型](https://zhuanlan.zhihu.com/p/27769667)\n\ntf.nn.seq2seq文件共实现了5个seq2seq函数，重点了解tf.nn.seq2seq.embedding_attention_seq2seq：\n\n```python\n# T代表time_steps, 时序长度\ndef embedding_attention_seq2seq(encoder_inputs,  # [T, batch_size] \n                                decoder_inputs,  # [T, batch_size]\n                                cell,\n                                num_encoder_symbols,\n                                num_decoder_symbols,\n                                embedding_size,\n                                num_heads=1,      # attention的抽头数量\n                                output_projection=None, #decoder的投影矩阵\n                                feed_previous=False,\n                                dtype=None,\n                                scope=None,\n                                initial_state_attention=False):\n```\n\n参数：\n\n### 2、[Transformer详解](https://zhuanlan.zhihu.com/p/44121378)\n\n​\t自attention机制提出后，加入attention的seq2seq模型在各个任务上都有了提升，所以现在的seq2seq模型指的都是结合rnn和attention的模型。之后google又提出了解决sequence to sequence问题的transformer模型，用全attention的结构代替了lstm，在翻译任务上取得了更好的成绩。下面主要介绍《Attention is all you need》这篇文章。\n\n#### 1.模型结构\n\n和大多数seq2seq模型一样，transformer结构也是由encoder和decoder组成，如图：![preview](https://pic1.zhimg.com/v2-4b53b731a961ee467928619d14a5fd44_r.jpg)\n\n##### 1.1 Encoder(编码)\n\n​\tEncoder由N=6个 相同的layer组成，layer指的就是上图左侧的单元，最左边有个“Nx\"，这里是x6个。每个Layer由两个sub-layer组成，分别是multi-head self-attention mechanism和fully connected feed-forward network。其中每个sub-layer都加了residual connection和normalisation，因此可以将sub-layer的输出表示为：\n\n![[公式]](https://www.zhihu.com/equation?tex=sub%5C_layer%5C_output+%3D+LayerNorm%28x%2B%28SubLayer%28x%29%29%29+%5C%5C)\n\n接下来按顺序解释一下这两个sub-layer：\n\n- **Multi-head self-attention**\n\n熟悉attention原理的知道，attention可由以下形式表示：\n\n![[公式]](https://www.zhihu.com/equation?tex=attention%5C_output+%3D+Attention%28Q%2C+K%2C+V%29+%5C%5C)\n\nmulti-head attention则是通过h个不同的**线性变换**对Q，K，V进行投影，最后将不同的attention结果拼接起来：\n\n![[公式]](https://www.zhihu.com/equation?tex=MultiHead%28Q%2C+K%2C+V%29+%3D+Concat%28head_1%2C+...%2C+head_h%29W%5EO+%5C%5C)\n\n![[公式]](https://www.zhihu.com/equation?tex=head_i+%3D+Attention%28QW_i%5EQ%2C+KW_i%5EK%2C+VW_i%5EV%29+%5C%5C)\n\nself-attention则是取Q，K，V相同。\n\n另外，文章中attention的计算采用了scaled dot-product，即：\n\n![[公式]](https://www.zhihu.com/equation?tex=Attention%28Q%2C+K%2C+V%29+%3D+softmax%28%5Cfrac%7BQK%5ET%7D%7B%5Csqrt%7Bd_k%7D%7D%29V+%5C%5C)\n\n作者同样提到了另一种复杂度相似但计算方法additive attention，在 ![[公式]](https://www.zhihu.com/equation?tex=d_k) 很小的时候和dot-product结果相似，![[公式]](https://www.zhihu.com/equation?tex=d_k)大的时候，如果不进行缩放则表现更好，但dot-product的计算速度更快，进行缩放后可减少影响（由于softmax使梯度过小，具体可见论文中的引用。\n\n- **Position-wise feed-forward networks**\n\n这层主要是提供**非线性变换**。Attention输出的维度是[bsz*seq_len, num_heads*head_size]，第二个sub-layer是个全连接层，之所以是position-wise是因为过线性层时每个位置i的变换参数是一样的。\n\n##### 1.2 Decoder（解码）\n\ndecoder和encoder的结构差不多，但是多了一个attention的sub-layer。\n\ndecoder的输入输出和解码过程：\n\n- 输出：对应i位置的输出词的概率分布\n\n- 输入：encoder的输出 & 对应i-1位置decoder的输出。所以中间的attention不是self-attention，它的K，V来自encoder，Q来自上一位置decoder的输出\n\n- 解码：**这里要特别注意一下，编码可以并行计算，一次性全部encoding出来，但解码不是一次把所有序列解出来的，而是像rnn一样一个一个解出来的**，因为要用上一个位置的输入当作attention的query\n\n  明确解码过程之后，新加的attention多加了一个mask，因为训练时的output都是ground truth，这样可以确保预测第i个位置时不会接触到未来的信息。\n\n  加了mask的attention原理如图（另附multi-head attention):\n\n  ![preview](https://pic3.zhimg.com/v2-df2ca1b7a60d829245b7b7c37f80a3aa_r.jpg)\n\n  ##### 1.3 Positional Encoding\n\n   除了最主要的Encoder和Decoder，还有数据预处理的部分。Transformer抛弃了rnn，而rnn最大的优点就是在时间序列上对数据的抽象，所以文章中作者提出两种Positional Encoding的方法，将encoding后的数据与embedding数据求和，加入了相对位置信息。\n\n  这里作者提到了两种方法：\n\n  1.用不同频率的sine和cosine函数直接计算\n\n  2.学习出一份positional embedding\n\n  经过实验发现两者的结果一样，所以最后选择了第一种方法，公式如下：\n\n  ![[公式]](https://www.zhihu.com/equation?tex=PE_%7B%28pos%2C+2i%29%7D+%3D+sin%28pos%2F10000%5E%7B2i%2Fd_%7Bmodel%7D%7D%29+%5C%5C)\n\n  ![[公式]](https://www.zhihu.com/equation?tex=PE_%7B%28pos%2C+2i%2B1%29%7D+%3D+cos%28pos%2F10000%5E%7B2i%2Fd_%7Bmodel%7D%7D%29+%5C%5C)\n\n  作者提到，方法1的好处有两点：\n\n  1.任意位置的 ![[公式]](https://www.zhihu.com/equation?tex=PE_%7Bpos%2Bk%7D) 都可以被 ![[公式]](https://www.zhihu.com/equation?tex=PE_%7Bpos%7D) 的线性函数表示，三角函数特性复习下：\n\n  ![[公式]](https://www.zhihu.com/equation?tex=cos%28%5Calpha%2B%5Cbeta%29+%3D+cos%28%5Calpha%29cos%28%5Cbeta%29-sin%28%5Calpha%29sin%28%5Cbeta%29+%5C%5C)\n\n  ![[公式]](https://www.zhihu.com/equation?tex=sin%28%5Calpha%2B%5Cbeta%29+%3D+sin%28%5Calpha%29cos%28%5Cbeta%29+%2B+cos%28%5Calpha%29sins%28%5Cbeta%29+%5C%5C)\n\n  2.如果是学习到的positional embedding，（个人认为，没看论文）会像词向量一样受限于词典大小。也就是只能学习到“位置2对应的向量是(1,1,1,2)”这样的表示。所以用三角公式**明显不受序列长度的限制**，也就是可以对 比所遇到序列的更长的序列 进行表示。\n\n#### 优点\n\n作者主要讲了以下三点：\n\n![preview](https://pic4.zhimg.com/v2-a36b4e3924ef8864716736f8f1e77233_r.jpg)\n\n1.Total computional complexity per layer（每层计算复杂度）\n\n2.Amount of computation that can be parallelized,as mesured by the minimum number of sequential operations required 作者用最小的序列化运算来测量可以被并行化的计算。也就是桌对于某个序列![[公式]](https://www.zhihu.com/equation?tex=x_1%2C+x_2%2C+...%2C+x_n) ，self-attention可以直接计算 ![[公式]](https://www.zhihu.com/equation?tex=x_i%2C+x_j) 的点乘结果，而rnn就必须按照顺序从 ![[公式]](https://www.zhihu.com/equation?tex=x_1) 计算到 ![[公式]](https://www.zhihu.com/equation?tex=x_n)\n\n3.Path length between long-range dependencies in the network\n\n这里Path length指的是要计算一个序列长度为n的信息要经过的路径长度。cnn需要增加卷积层数来扩大视野，rnn需要从1到n逐个进行计算，而self-attention只需要一步矩阵计算就可以。所以也可以看出，self-attention可以比rnn更好地解决长时依赖问题。当然如果计算量太大，比如序列长度n>序列维度d这种情况，也可以用窗口限制self-attention的计算数量\n\n4.另外，从作者在附录中给出的例子可以看出，self-attention模型更可解释，attention结果的分布表明了该模型学习到了一些语法和语义信息\n\n![preview](https://pic3.zhimg.com/v2-d69547987a6510f22171b35c6e3f7e7e_r.jpg)\n\n#### 缺点\n\n原文中没有提到，后来在Universal Transformers中提出的，主要有两点：\n\n1.实践上：有些rnn轻易可以解决的问题transformer没做到，比如复制string，或者推理时碰到的sequence长度比训练时更长（因为碰到了没见过的position embedding)\n\n2.理论上：transformers非computational universal([图灵完备](https://www.zhihu.com/question/20115374/answer/288346717))，认为无法实现while循环\n\n#### 总结\n\nTransformer是一个用纯attention搭建的模型，不仅计算速度更快，在翻译任务上也获得了更好的结果。google现在的翻译应该是在此基础上做的，数据量大可能用transformer好一些，小的话还是继续有用rnn-based model。\n\n## BERT模型结构\n\n#### 模型的构成元素是Transformer，BERT模型的结构如下图最左：\n\n![preview](https://pic1.zhimg.com/v2-d942b566bde7c44704b7d03a1b596c0c_r.jpg)\n\n对比OpenAI GPT(Generative pre-trained transformer)，BERT是双向的Transformer block连接；就像单向rnn和双向rnn的区别，直觉上来讲效果会好一些。\n\n对比ELMo，虽然都是“双向”，但目标函数其实是不同的。ELMo是分别以![[公式]](https://www.zhihu.com/equation?tex=P%28w_i%7C+w_1%2C+...w_%7Bi-1%7D%29) 和 ![[公式]](https://www.zhihu.com/equation?tex=P%28w_i%7Cw_%7Bi%2B1%7D%2C+...w_n%29) 作为目标函数，独立训练处两个representation然后拼接，而BERT则是以 ![[公式]](https://www.zhihu.com/equation?tex=P%28w_i%7Cw_1%2C++...%2Cw_%7Bi-1%7D%2C+w_%7Bi%2B1%7D%2C...%2Cw_n%29) 作为目标函数训练LM。\n\n### Embedding\n\n#### 这里的Embedding由三种[Embedding](https://blog.csdn.net/qq_35799003/article/details/84780289)求和而成：\n\n![preview](https://pic2.zhimg.com/v2-11505b394299037e999d12997e9d1789_r.jpg)\n\n其中：\n\n·Token Embeddings是词向量，第一个单词是CLS标志，可以用于之后的分类任务\n\n·Segment Embeddings是用来区别两种句子，因为预训练不光做LM还要做以两个句子为输入的分类任务\n\n·Position Embeddings和之前文章中的Transformer不一样，不是三角函数而是学习出来的\n\n### 预训练第一步：Masked LM\n\n​\t第一步预训练的目标就是做语言模型，从上文模型机构中看到了这个模型的不同，即bidirectional(双向的)。如果使用预训练模型处理其他任务，那人们想要的肯定不止某个词左边的信息，二十左右两边的信息。而考虑到这点的模型ELMo只是将left-to-right和right-to-left分别训练拼接起来。直觉上来讲我们其实想要一个deeply bidirectional的模型，但是普通的LM又无法做到，因为在训练时可能会“穿越”，所以作者用了一个加mask的技巧。\n\n​\t在训练过程中作者随机mask 15%的token，而不是把像cbow一样把每个词都预测一遍。**最终的损失函数只计算被mask掉那个token。**\n\n​\tMask如何做也是有技巧的，如果一直用标记[MASK]代替（在实际预测时是碰不到这个标记的）会影响模型，所以随机mask的时候10%的单词会被替代成其他单词，10%的单词不替换，剩下80%才被替换为[MASK]。具体为什么这么分配，作者没有说。要注意的是Masked LM预训练阶段模型是不知道真正被mask的是哪个词，所以模型每个词都要关注。\n\n​\t因为序列长度太大（512）会影响训练速度，所以90%的steps都用seq_len=128训练，余下的10%步数训练512长度的输入。\n\n### 预训练第二步：Next Sentence Prediction\n\n​\t因为涉及到QA和NLI之类的任务，增加了第二个预训练任务，目的是让模型理解两个句子之间的联系。训练的输入是句子A和B，B有一半的几率是A的下一句，输入这两个句子，模型预测B是不是A的下一句。预训练的时候可以达到97-98%的准确度。\n\n​\t注意：作者特意说了预料的选取很关键，要选用document-level的而不是sentence-level的，这样可以具备抽象连续长序列特征的能力。\n\n### Fine-tunning(微调)\n\n分类：对于sequence-level的分类任务，BERT直接取第一个[CLS]token的final hidden state ![[公式]](https://www.zhihu.com/equation?tex=C%5Cin%5CRe%5EH) ，加一层权重 ![[公式]](https://www.zhihu.com/equation?tex=W%5Cin%5CRe%5E%7BK%5Ctimes+H%7D) 后softmax预测label proba： ![[公式]](https://www.zhihu.com/equation?tex=P%3Dsoftmax%28CW%5ET%29+%5C%5C)\n\n其他预测任务需要进行一些调整，如图：![preview](https://pic2.zhimg.com/v2-b054e303cdafa0ce41ad761d5d0314e1_r.jpg)\n\n可以调整的参数和取值范围有：\n\n- Batch size: 16, 32\n- Learning rate (Adam): 5e-5, 3e-5, 2e-5\n- Number of epochs: 3, 4\n\n因为大部分参数都和预训练时一样，精调会快一些，所以作者推荐多试一些参数。\n\n### 优点\n\nBERT是截至2018年10月的最新state of the art模型，通过预训练和精调横扫了11项NLP任务，这首先就是最大的优点了。而且它还用的是Transformer，也就是相对rnn更加高效、能捕捉更长距离的依赖。对比起之前的预训练模型，它捕捉到的是真正意义上的bidirectional context信息。\n\n### 缺点\n\n作者在文中主要提到的就是MLM预训练时的mask问题：\n\n1. [MASK]标记在实际预测中不会出现，训练时用过多[MASK]影响模型表现\n2. 每个batch只有15%的token被预测，所以BERT收敛得比left-to-right模型要慢（它们会预测每个token）\n\n### 总结\n\n一遍读下来，感觉用到的都是现有的东西，可没想到效果会这么好，而别人又没想到。不过文章中没有具体解释的很多点可以看出这样出色的结果也是通过不断地实验得出的，而且训练的数据也比差不多结构的OpenAI GPT多，所以数据、模型结构，都是不可或缺的东西。\n\n​\t"},{"title":"leetcode-时间复杂度问题","url":"/2020/01/17/leetcode-时间复杂度问题/","content":"# LeetCode算法\n\n## 时间复杂度\n\n![什么是大O](C:\\Users\\zhangdi\\AppData\\Roaming\\Typora\\typora-user-images\\image-20200117171132054.png)\n\n![image-20200117171258470](C:\\Users\\zhangdi\\AppData\\Roaming\\Typora\\typora-user-images\\image-20200117171258470.png)\n\nn=1000时，执行算法A与执行算法B速度相同，＞1000后，A比B速度快。当数据规模大时，算法时间复杂度的差距还是很大的。\n\n**大O表示的是时间的上界。**在业界，我们就用O来表示算法执行的最低上界，一般不会说归并排序算法是O(n^2)，而是O(nlogn)的。\n\nO(nlogn+n)=O(nlogn) \n\nO(nlogn+n^2)=O(n^2)\n\nO(AlogA+B) A和B没有关系\n\n对邻接表实现的图进行遍历：时间复杂度O(V+E) 顶点个数V 边个数E\n\n### 一个时间复杂度的问题：\n\n有一个字符串数组，将数组中的每一个字符串按照字母序排序；之后再将整个字符串数组按照字典序排序。整个操作的时间复杂度？\n\n假设最长的字符串长度为s；数组中有n个字符串。对每个字符串排序：O(slogs)，将数组中的每一个字符串按照字母序排序：O(n*slog(s))，将整个字符串数组按照字典序排序:O(s*nlog(n))\n\nO(n*slog(s))+O(s*nlog(n))=O(n*s*logs+s*n*logn)=O(n*s*(logs+logn))\n\n很多时候算法复杂度是用例相关的。大多是情况还是关注平均情况。\n\n![image-20200117180908231](C:\\Users\\zhangdi\\AppData\\Roaming\\Typora\\typora-user-images\\image-20200117180908231.png)\n\n## 对数据规模有一个概念\n\n![image-20200117185209920](C:\\Users\\zhangdi\\AppData\\Roaming\\Typora\\typora-user-images\\image-20200117185209920.png)\n\n计算从10到10^9所用时间，运算结果：\n\n![image-20200117185320275](C:\\Users\\zhangdi\\AppData\\Roaming\\Typora\\typora-user-images\\image-20200117185320275.png)\n\n对数据规模有个概念：如果想在1s之内解决问题，用O(n^2)的算法可以处理大约10^4级别的数据；O(n)大约10^8级别的数据；O(nlogn)大约10^7级别的数据。对于多大规模的数据，我应该采用哪种时间复杂度来解决。\n\n空间复杂度：多开一个辅助的数组：O(n);多开一个辅助的二维数组：O(n^2)；多开一个常数：O(1)。\n\n递归调用是有空间代价的\n\n![image-20200117190253899](C:\\Users\\zhangdi\\AppData\\Roaming\\Typora\\typora-user-images\\image-20200117190253899.png)\n\n开了两个空间。\n\n![image-20200117190314500](C:\\Users\\zhangdi\\AppData\\Roaming\\Typora\\typora-user-images\\image-20200117190314500.png)\n\n递归调用，所以空间复杂度是O(n)。递归深度是n，系统栈中就要装n个状态。\n\n在递归调用中，递归深度是多少，整个递归过程就占用多少空间复杂度。\n\n## 常见的复杂度分析\n\n### O(1)\n\n```\nvoid swapTwoInts( int &a, int &b){\n    int temp = a;\n\ta=b;\n\tb=temp;\n}\n```\n\n\n\n### O(n)\n\n`void reverse( string &s ){`\n\n` int n = s.size();` \n\n`for (int i = 0; i< n/2; i++)`\n\n`swap(s[i],s[n-1-i]);`\n\n`}`\n\n进行了1/2*n次swap操作\n\n### O(n^2)\n\n```\nvoid selectionSort( int arr[], int n){\n\tfor(int i = 0;i < n; i++){\n\tint minIndex =i;\t\n    for(int j = i+1;j<n;j++)\n    if(arr[j]<arr[minIndex])\n\tminIndex = j;\n    swap(arr[i],arr[minIndex]);\t\n}\n}\n```\n\nj执行了(n-1)+(n-2)+(n-3)+...+0=(0+n-1)*n/2=(1/2)n*(n-1)=1/2*n^2-1/2*n=O(n^2)。双重循环，每重循环都和n有关。\n\n![image-20200117191739177](C:\\Users\\zhangdi\\AppData\\Roaming\\Typora\\typora-user-images\\image-20200117191739177.png)\n\n对于第二重循环,j的执行次数是固定的\n\n### O(logn)\n\n```\nint binarySearch(int arr[],int n,int target){\n\tint l=0,r=n-1;\n\twhile (l<=r){\n\tint mid = l+(r-l)/2; \n\tif(arr[mid]==target) return mid;\n\tif(arr[mid]>target) r=mid-1;\t\n\telse l = mid +1;`\t\n}\nreturn -1;\n}\n```\n\n二分查找法：每次查找要是没查找到就扔掉一半元素。在n个元素中找，在n/2个元素中找，在n/4个元素中找...在1个元素中找。n经过几次“除以2”操作后，等于1？log2n=O(logn)\n\n整形转成字符串：（只考虑num为正数）`string intToString(int num){`\n\n```\nstring s =\"\";\nwhile(num){\ns+='0'+num%10;\nnum /= 10;\n}\nreverse(s);\nreturn s;\n}\n```\n\nn经过几次“除以10”操作后，等于0？log10n=O(logn)\n\n![image-20200117193130571](C:\\Users\\zhangdi\\AppData\\Roaming\\Typora\\typora-user-images\\image-20200117193130571.png)\n\n我们说一个算法是对数复杂度的，通常忽略这个对数的底\n\n```\nvoid hello(int n){\n\tfor(int sz=1;sz<n;sz+=sz)//这里sz+=sz是指每次乘以二，这重循环的复杂度是对数级别的\n\tfor (int i=1;i<n;i++)\t\n\tcout<<\"Hello,Algorithm!\"<<endl;\t\n}\n```\n\n虽然是双重循环，总共的复杂度为O(nlogn)。要注意循环的起始点和终止点，每次循环增量的变化。\n\n```\nbool isPrime(int n){\n\tfor(int x=2;x*x<=n;x++)\t//O(sqrt(n))\n\tif(n%x==0)\nreturn false;\nreturn true;\n}\n```\n\n判断一个数是不是素数？\n\n## 复杂度实验\n\n我们自以为写出了一个O(nlogn)的算法，但实际是O(n^2)的算法？\n\n实验，观察趋势。每次将数据规模提高两倍，观察\n\n```\n#include <iostream>\n#include <cassert>\nusing namespace std;\nnamespace MyAlgorithmTester{\n// O(logn)二分法\nint binarySearch(int arr[],int n,int target){\n\tint l=0,r=n-1;\n\twhile (l<=r){\t\n    int mid = l+(r-l)/2;\n\tif(arr[mid]==target) return mid;\n\tif(arr[mid]>target) r=mid-1;\t\n\telse l = mid +1;\t\n}\nreturn -1;\n}\n\n//O(n) 寻找最大值\nint findMax(int arr[],int n){\n    assert(n>0);\n    int res = arr[0];\n    for(int i = 1; i < n; i++)\n        if(arr[i]>res)\n        res=arr[i];\n}\n\n//O(nlogn) 自顶向上的归并排序\nvoid _merge(int arr[],int l,int mid,int r,int aux[]){\n    int i = l,j = mid+1;\n    for(int k=l;k<=r;k++){\n        if(i>mid)\n        {arr[k] = aux[j]; j++;}\n        else if (j>r)\n        {arr[k] = aux[i]; i++;}\n        else if(aux[i]<aux[j])\n        {arr[k] = aux[i]; i++;}\n        else\n        {arr[k] = aux[j]; j++}\n    }\n}\n\n//O(n^2) 选择排序算法\nvoid selectionSort(int arr[],int n){\n    for (int i = 0;i < n; i++){\n        int minIndex = i;\n        for(int j = i + 1; j < n; j++)\n            if(arr[j]<arr[minIndex])\n                minIndex = j;\n        swap(arr[i],arr[minIndex]);\n    }\n    return;\n}\n}\n\n\n\n```\n\nn乘以2以后，时间性能log2n/logn=(log2+logn)/logn=1+log2/logn。如果有个算法复杂度是logn级别，当数据扩大为2倍，效率会提高一点几倍。\n\n将顺序查找转换为二分查找，当数据规模很大时会节省很多时间\n"},{"title":"CS224n课程记录","url":"/2020/01/15/CS224n课程记录/","content":"# CS224N课程笔记-第一周\n\n寒假开始了，作为我的第一篇正式博客，我当然要用轻便快捷的[markdown语法](https://www.runoob.com/markdown/md-tutorial.html)来写了。作为一个身兼大创与考研重压的菜虚鸭，寒假的学习一定要搞起来。冲冲冲！\n\n## 课程介绍\n\n### Standford University CS224n《深度学习与NLP》课程\n\n自然语言是人类智慧的结晶，自然语言处理是人工智能中最为困难的问题之一，而对自然语言处理的研究也是充满魅力和挑战的。 通过经典的斯坦福cs224n教程，让我们一起和自然语言处理共舞！\n\n#### 第一周内容：\n\n观看自然语言处理课学习绪论，了解深度学习的概括和应用案例以及训练营后续的一些学习安排。\n\n##### 绪论\n\n[视频地址](https://m.weike.fm/lecture/10194068)\n\n###### 机器学习三板斧：好比把大象放入冰箱 \n\nStep1:define a set of function\n\nStep2:goodness of function \n\nStep3:pick the best function\n\n###### 深度学习=用多层次神经网络来找到函数 \n\n深度学习三板斧：\n\nStep1:Neural Network\n\nStep2:goodness of function\n\nStep3:pick the best function\n\n###### 深度学习VS机器学习\n\n深度学习对比机器学习能够实现端到端的模型，中间减少人为的参与\n\nNLP领域：（1）情绪分析（2）问答系统（3）文本处理\n\nAI大咖：[Yann LeCun](https://blog.csdn.net/hacker_long/article/details/89609367) \n\n##### lecture01 \n\n[视频地址](https://www.bilibili.com/video/av41393758/?spm_id_from=333.788.videocard.0)\n\n计算机如何处理词语的意思：过去几个世纪里一直用的是分类词典。计算语言学中常见的方式是WordNet那样的词库。比如NLTK中可以通过WordNet查询熊猫的hypernyms (is-a，上位词)，得到“食肉动物”“动物”之类的上位词。也可以查询“good”的同义词——“just品格好”“ripe熟了”。\n\ndiscrete representation的问题：（1）缺少新词（2）主观化（3）需要耗费大量人力去整理（4）无法计算准确的词语相似度\n\n词语只是词表长度的one-hot(独热)向量，这是一种localist representation\n\n\n"},{"title":"Hello World","url":"/2020/01/13/hello-world/","content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n"}]